\section{Introducción}
\label{Intro}

\noindent
Un agente (una persona, un animal, un robot o un programa) necesita tomar decisiones en un ambiente desconocido. Sus decisiones tienen consecuencias inmediatas, de mediano y de largo plazo. En todos los casos, se desea maximizar algún tipo de beneficio (comida, trabajo realizado, paquetes enviados) y minimizar  algún costo (tiempo, dinero, energía). Ante lo incierto del entorno y del resultado esperado de las acciones a tomar, el agente tiene que decidir si actuar en base al conocimiento que tiene, o invertir tiempo en explorar y adquirir más información.

En los '80 y '90, ideas que venían de la psicología, teoría de control, optimización y microeconomía dieron nacimiento al campo de RL. Unos de sus fundadores, Rich Sutton (junto con Andy Barto), decía: \textit{«la hipótesis del aprendizaje por refuerzos es que aquello que entendemos por comportamiento inteligente surge de un agente que intenta maximizar la esperanza a largo plazo de la suma de \textit{\textbf{refuerzos}} recibidos (usualmente en un ambiente desconocido)»}. La idea es que nuestros agentes aprendan a comportarse de manera (cuasi-)óptima solamente guiados por su afán de maximizar una señal de refuerzo, pero sin la presencia de un experto que les indique qué acciones tomar en cada momento.

El formalismo de RL ha tenido un éxito significativo en términos de algoritmos y aplicaciones que produjo en sus 25 años de vida. En robótica, donde tuvo un especial éxito, aplicaciones de RL incluyen helicópteros a control remoto que pueden volar solos y hasta en forma invertida, robots cuadrúpedos que sortean obstáculos complejos, perros robots que encuentra la manera más rápida posible de correr. RL también tuve un fuerte impacto en juegos como el backgammon, donde el algoritmo TD-Gammon logró derrotar a los mejores jugadores humanos, y hoy en día está teniendo mucho éxito en juegos como el Go, con programas como RLGo que juegan en tableros de $9$\texttt{x}$9$ a nivel de Masters humanos. Los casos anteriores, sin embargo, cuentan parte de la historia. RL recibe muchas críticas: que no es escalable, que no es confiable, que los algoritmmos tardan demasiado en converger.

Este trabajo está organizado de la siguiente manera: sección 2, trata el formalismo estándar de RL; sección 3, provee una explicación básica y con ejemplos del Q-Learning algoritmo; sección 4, se ocupa de RL-glue y RL-library (proyectos open-source que permitieron la implementación del algoritmo); sección 5, habla de la implementación en sí, usada para el tetris; y por último, las conclusiones.